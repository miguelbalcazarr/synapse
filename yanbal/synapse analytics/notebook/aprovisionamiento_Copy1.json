{
	"name": "aprovisionamiento_Copy1",
	"properties": {
		"folder": {
			"name": "utils"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "kyndryl",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "cad5d2b6-958d-4c3e-96f5-7fb2ba9e99b6"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/e4516ad2-d1ef-463b-817c-b2a88c0b12bd/resourceGroups/rg-kyndryl/providers/Microsoft.Synapse/workspaces/synwkyndryl/bigDataPools/kyndryl",
				"name": "kyndryl",
				"type": "Spark",
				"endpoint": "https://synwkyndryl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/kyndryl",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# se define parametros de la base de datos\r\n",
					"name_database = \"staging\"\r\n",
					"location_database = \"abfss://staging@stakyndryl.dfs.core.windows.net/database/staging/\"\r\n",
					"\r\n",
					"# se define parametros de la tabla\r\n",
					"name_table = \"SalesOrderHeader\"\r\n",
					"location_table = \"abfss://staging@stakyndryl.dfs.core.windows.net/SalesOrderHeader\"\r\n",
					"partition_table = \"auditDate\"\r\n",
					"schema_table = \"\"\"SalesOrderID string,\r\n",
					"RevisionNumber string,\r\n",
					"OrderDate string,\r\n",
					"DueDate string,\r\n",
					"ShipDate string,\r\n",
					"Status string,\r\n",
					"OnlineOrderFlag string,\r\n",
					"SalesOrderNumber string,\r\n",
					"PurchaseOrderNumber string,\r\n",
					"AccountNumber string,    \r\n",
					"CustomerID string,\r\n",
					"ShipToAddressID string,\r\n",
					"BillToAddressID string,\r\n",
					"ShipMethod string,     \r\n",
					"CreditCardApprovalCode string,\r\n",
					"SubTotal string,\r\n",
					"TaxAmt string,\r\n",
					"Freight string,\r\n",
					"TotalDue string,\r\n",
					"Comment string, \r\n",
					"rowguid string,\r\n",
					"ModifiedDate string,\r\n",
					"auditDate int\"\"\"\r\n",
					"\r\n",
					""
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"# se crea database\r\n",
					"spark.sql(f\"create database if not exists {name_database} LOCATION '{location_database}'\")\r\n",
					"\r\n",
					"print(f\"Database {name_database} created successfully\")\r\n",
					""
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# se crea tabla\r\n",
					"spark.sql(f\"\"\"create table if not exists {name_database}.{name_table} ({schema_table}) using delta location '{location_table}'  PARTITIONED BY({partition_table})\"\"\")\r\n",
					"\r\n",
					"print(f\"Table {name_database}.{name_table} created successfully\")"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Especificar la tabla SQL\r\n",
					"tabla_sql = \"staging.salesorderheader\"\r\n",
					"\r\n",
					"# Leer todos los datos de la tabla SQL en un DataFrame\r\n",
					"df = spark.sql(f\"SELECT * FROM {tabla_sql}\")\r\n",
					"\r\n",
					"# Especificar la ruta en el sistema de archivos de Databricks\r\n",
					"ruta_parquet = \"abfss://staging@stakyndryl.dfs.core.windows.net/parquet/\"\r\n",
					"ruta_csv = \"abfss://staging@stakyndryl.dfs.core.windows.net/csv/\"\r\n",
					"\r\n",
					"# Escribir el DataFrame en formato Parquet con un nombre de archivo espec√≠fico\r\n",
					"nombre_archivo_parquet = \"salesorderheader.parquet\"\r\n",
					"#df = df.repartition(2)\r\n",
					"df.coalesce(1).write.parquet(ruta_parquet)\r\n",
					"#df.write.csv(ruta_csv)"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"ruta_parquet = \"abfss://staging@stakyndryl.dfs.core.windows.net/parquet/data2.parquet\"\r\n",
					"nueva_ruta = \"abfss://staging@stakyndryl.dfs.core.windows.net/parquet/data3.parquet\"\r\n",
					"mssparkutils.fs.mv(ruta_parquet,nueva_ruta)"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"df = spark.read.parquet(\"abfss://staging@stakyndryl.dfs.core.windows.net/parquet/data.parquet\")\r\n",
					"display(df)"
				],
				"execution_count": 16
			}
		]
	}
}