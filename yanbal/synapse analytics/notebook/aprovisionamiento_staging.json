{
	"name": "aprovisionamiento_staging",
	"properties": {
		"folder": {
			"name": "staging"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "kyndryl",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "53830a61-9cae-4d6b-9168-4a4e25493118"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/e4516ad2-d1ef-463b-817c-b2a88c0b12bd/resourceGroups/rg-kyndryl/providers/Microsoft.Synapse/workspaces/synwkyndryl/bigDataPools/kyndryl",
				"name": "kyndryl",
				"type": "Spark",
				"endpoint": "https://synwkyndryl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/kyndryl",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#database\r\n",
					"name_database = \"staging\"\r\n",
					"location_database = \"abfss://staging@stakyndryl.dfs.core.windows.net/database/staging/\"\r\n",
					"\r\n",
					"#table\r\n",
					"name_table = \"SalesOrderHeader2\"\r\n",
					"location_table = \"abfss://staging@stakyndryl.dfs.core.windows.net/SalesOrderHeader2\"\r\n",
					"schema_table = \"\"\"\r\n",
					"SalesOrderID string,\r\n",
					"RevisionNumber string,\r\n",
					"OrderDate string,\r\n",
					"DueDate string,\r\n",
					"ShipDate string,\r\n",
					"Status string,\r\n",
					"OnlineOrderFlag string,\r\n",
					"SalesOrderNumber string,\r\n",
					"PurchaseOrderNumber string,\r\n",
					"AccountNumber string,    \r\n",
					"CustomerID string,\r\n",
					"ShipToAddressID string,\r\n",
					"BillToAddressID string,\r\n",
					"ShipMethod string,     \r\n",
					"CreditCardApprovalCode string,\r\n",
					"SubTotal string,\r\n",
					"TaxAmt string,\r\n",
					"Freight string,\r\n",
					"TotalDue string,\r\n",
					"Comment string, \r\n",
					"rowguid string,\r\n",
					"ModifiedDate string,\r\n",
					"auditDate timestamp\r\n",
					"\"\"\""
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"spark.sql(f\"create database if not exists {name_database} LOCATION '{location_database}'\")\r\n",
					"\r\n",
					"print(f\"Database {name_database} created successfully\")\r\n",
					""
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"spark.sql(f\"\"\"create table if not exists {name_database}.{name_table} ({schema_table}) using delta location '{location_table}'\"\"\")\r\n",
					"\r\n",
					"print(f\"Table {name_database}.{name_table} created successfully\")"
				],
				"execution_count": 39
			}
		]
	}
}