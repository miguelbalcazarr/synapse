{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "synwkyndryl"
		},
		"synwkyndryl-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synwkyndryl-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synwkyndryl.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"synwkyndryl-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://stakyndryl.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/ppl_data_parameters')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "aprovisionamiento",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "write_csv_source_to_landing",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "aprovisionamiento",
								"type": "NotebookReference"
							},
							"parameters": {
								"name_database": {
									"value": "staging",
									"type": "string"
								},
								"location_database": {
									"value": "abfss://staging@stakyndryl.dfs.core.windows.net/database/staging/",
									"type": "string"
								},
								"name_table": {
									"value": "SalesOrderHeader",
									"type": "string"
								},
								"location_table": {
									"value": "abfss://staging@stakyndryl.dfs.core.windows.net/SalesOrderHeader",
									"type": "string"
								},
								"schema_table": {
									"value": "SalesOrderID string,\nRevisionNumber string,\nOrderDate string,\nDueDate string,\nShipDate string,\nStatus string,\nOnlineOrderFlag string,\nSalesOrderNumber string,\nPurchaseOrderNumber string,\nAccountNumber string,    \nCustomerID string,\nShipToAddressID string,\nBillToAddressID string,\nShipMethod string,     \nCreditCardApprovalCode string,\nSubTotal string,\nTaxAmt string,\nFreight string,\nTotalDue string,\nComment string, \nrowguid string,\nModifiedDate string,\nauditDate timestamp",
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "kyndryl",
								"type": "BigDataPoolReference"
							},
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					},
					{
						"name": "write_csv_source_to_landing",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "write_csv_source_to_landing",
								"type": "NotebookReference"
							},
							"parameters": {
								"name_database": {
									"value": "kyndryldb",
									"type": "string"
								},
								"query": {
									"value": "select * from SalesLT.SalesOrderHeader",
									"type": "string"
								},
								"mode_csv": {
									"value": "overwrite",
									"type": "string"
								},
								"header_csv": {
									"value": "True",
									"type": "string"
								},
								"delimiter_csv": {
									"value": "|",
									"type": "string"
								},
								"container_csv": {
									"value": "stakyndryl",
									"type": "string"
								},
								"path_csv": {
									"value": "SalesOrderHeader",
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "kyndryl",
								"type": "BigDataPoolReference"
							},
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/aprovisionamiento')]",
				"[concat(variables('workspaceId'), '/bigDataPools/kyndryl')]",
				"[concat(variables('workspaceId'), '/notebooks/write_csv_source_to_landing')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synwkyndryl-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synwkyndryl-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synwkyndryl-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synwkyndryl-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 1')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT * FROM [ods].[dbo].[salesorderheader]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "ods",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SQL script 2')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT TOP (100) * FROM [staging].[dbo].[salesorderheader]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "staging",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/aprovisionamiento')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "utils"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "kyndryl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "6cf5cd57-6118-415b-8aa8-33cbe4102b52"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4516ad2-d1ef-463b-817c-b2a88c0b12bd/resourceGroups/rg-kyndryl/providers/Microsoft.Synapse/workspaces/synwkyndryl/bigDataPools/kyndryl",
						"name": "kyndryl",
						"type": "Spark",
						"endpoint": "https://synwkyndryl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/kyndryl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# se define parametros de la base de datos\r\n",
							"name_database = \"\"\r\n",
							"location_database = \"\"\r\n",
							"\r\n",
							"# se define parametros de la tabla\r\n",
							"name_table = \"\"\r\n",
							"location_table = \"\"\r\n",
							"schema_table = \"\"\"\"\"\""
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# se crea database\r\n",
							"spark.sql(f\"create database if not exists {name_database} LOCATION '{location_database}'\")\r\n",
							"\r\n",
							"print(f\"Database {name_database} created successfully\")\r\n",
							""
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# se crea tabla\r\n",
							"spark.sql(f\"\"\"create table if not exists {name_database}.{name_table} ({schema_table}) using delta location '{location_table}'\"\"\")\r\n",
							"\r\n",
							"print(f\"Table {name_database}.{name_table} created successfully\")"
						],
						"outputs": [],
						"execution_count": 39
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/aprovisionamiento_ods')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ods"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "kyndryl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "93696a23-48d1-4151-b771-1f8ee413b746"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4516ad2-d1ef-463b-817c-b2a88c0b12bd/resourceGroups/rg-kyndryl/providers/Microsoft.Synapse/workspaces/synwkyndryl/bigDataPools/kyndryl",
						"name": "kyndryl",
						"type": "Spark",
						"endpoint": "https://synwkyndryl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/kyndryl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"create database if not exists ods LOCATION \"abfss://ods@stakyndryl.dfs.core.windows.net/database/ods/\""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"create table if not exists ods.SalesOrderHeader (\r\n",
							"    SalesOrderID int,\r\n",
							"\tRevisionNumber tinyint,\r\n",
							"\tOrderDate timestamp,\r\n",
							"\tDueDate timestamp,\r\n",
							"\tShipDate timestamp,\r\n",
							"\tStatus tinyint,\r\n",
							"\tOnlineOrderFlag boolean,\r\n",
							"\tSalesOrderNumber string,\r\n",
							"\tPurchaseOrderNumber string,\r\n",
							"\tAccountNumber string,    \r\n",
							"\tCustomerID int,\r\n",
							"\tShipToAddressID int,\r\n",
							"\tBillToAddressID int,\r\n",
							"\tShipMethod string,     \r\n",
							"\tCreditCardApprovalCode string,\r\n",
							"\tSubTotal decimal(18,4),\r\n",
							"\tTaxAmt decimal(18,4),\r\n",
							"\tFreight decimal(18,4),\r\n",
							"\tTotalDue decimal(18,4),\r\n",
							"\tComment string, \r\n",
							"\trowguid string,\r\n",
							"\tModifiedDate timestamp,\r\n",
							"\tauditDate timestamp\r\n",
							")\r\n",
							"using delta\r\n",
							"location \"abfss://ods@stakyndryl.dfs.core.windows.net/SalesOrderHeader\""
						],
						"outputs": [],
						"execution_count": 3
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/utils')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "utils"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "kyndryl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "417ec5f5-003a-4cff-a268-2d270bcb6267"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4516ad2-d1ef-463b-817c-b2a88c0b12bd/resourceGroups/rg-kyndryl/providers/Microsoft.Synapse/workspaces/synwkyndryl/bigDataPools/kyndryl",
						"name": "kyndryl",
						"type": "Spark",
						"endpoint": "https://synwkyndryl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/kyndryl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ------------------------------------------------------------------------------------------\r\n",
							"# se importa de librerias\r\n",
							"# ------------------------------------------------------------------------------------------\r\n",
							"\r\n",
							"from pyspark.sql.functions import *\r\n",
							"from pyspark.sql.types import *"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ------------------------------------------------------------------------------------------\r\n",
							"# parametros globales\r\n",
							"# ------------------------------------------------------------------------------------------\r\n",
							"\r\n",
							"storage_account = \"stakyndryl\""
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ------------------------------------------------------------------------------------------\r\n",
							"# (Solo para uso en desarrollo)Borra tabla y path\r\n",
							"# ------------------------------------------------------------------------------------------\r\n",
							"\r\n",
							"def delete_table(table, folder_path):\r\n",
							"    spark.sql(f\"drop table if exists {table}\")\r\n",
							"    \r\n",
							"    if mssparkutils.fs.exists(folder_path):\r\n",
							"        mssparkutils.fs.rm(folder_path, True)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# ------------------------------------------------------------------------------------------\r\n",
							"# se lee una query de Azure SQL\r\n",
							"# ------------------------------------------------------------------------------------------\r\n",
							"# parametros:\r\n",
							"#   db : nombre de base de datos\r\n",
							"#   query : query sql\r\n",
							"\r\n",
							"def read_sql(db, query):\r\n",
							"    server = mssparkutils.credentials.getSecret('kvkyndryl','azuresqlserver')\r\n",
							"    url = f\"{server}{db}\"\r\n",
							"    user = mssparkutils.credentials.getSecret('kvkyndryl',\"azuresqluser\")\r\n",
							"    #principal_client_id = mssparkutils.credentials.getSecret('kvkyndryl',\"principalclientid\")\r\n",
							"    #principal_secret = mssparkutils.credentials.getSecret('kvkyndryl',\"principalsecret\")\r\n",
							"    password = mssparkutils.credentials.getSecret('kvkyndryl',\"azuresqlpassword\")\r\n",
							"\r\n",
							"    df = spark.read \\\r\n",
							"    .format(\"com.microsoft.sqlserver.jdbc.spark\") \\\r\n",
							"    .option(\"url\", url) \\\r\n",
							"    .option(\"query\", query) \\\r\n",
							"    .option(\"user\", user) \\\r\n",
							"    .option(\"password\", password).load()\r\n",
							"\r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ------------------------------------------------------------------------------------------\r\n",
							"# se escribe csv\r\n",
							"# ------------------------------------------------------------------------------------------\r\n",
							"# parametros:\r\n",
							"#   df : dataframe a escribir\r\n",
							"#   mode : modo de escritura [\"append\", \"overwrite\", \"ignore\"]\r\n",
							"#   header : encabezado [True, False]\r\n",
							"#   delimiter : delimitado [\",\", \"|\", \" \", \";\"]\r\n",
							"#   container : nombre de contenedor en ADLS\r\n",
							"#   path : ruta donde se guardará el csv [\"abfss://contenedor@nombre_adls.dfs.core.windows.net/?/nombre_tabla\"]\r\n",
							"\r\n",
							"def write_csv(df, mode, header, delimiter,container, path):\r\n",
							"    df.write.format(\"csv\") \\\r\n",
							"    .mode(mode) \\\r\n",
							"    .option(\"header\", header) \\\r\n",
							"    .option(\"delimiter\", delimiter) \\\r\n",
							"    .save(f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{path}\")"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ------------------------------------------------------------------------------------------\r\n",
							"# se lee csv\r\n",
							"# ------------------------------------------------------------------------------------------\r\n",
							"# parametros:\r\n",
							"#   header : encabezado [True, False]\r\n",
							"#   delimiter : delimitado [\",\", \"|\", \" \", \";\"]\r\n",
							"#   container : nombre de contenedor en ADLS\r\n",
							"#   path : ruta donde se leera el csv [\"abfss://contenedor@nombre_adls.dfs.core.windows.net/?/nombre_tabla\"]\r\n",
							"\r\n",
							"def read_csv(header, delimiter, container, path):\r\n",
							"    df = spark.read \\\r\n",
							"    .option(\"header\",header) \\\r\n",
							"    .option(\"delimiter\",delimiter) \\\r\n",
							"    .csv(f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{path}\")\r\n",
							"\r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ------------------------------------------------------------------------------------------\r\n",
							"# se escribe delta\r\n",
							"# ------------------------------------------------------------------------------------------\r\n",
							"# parametros:\r\n",
							"#   df : dataframe a escribir\r\n",
							"#   mode : modo de escritura [\"append\", \"overwrite\", \"ignore\"]\r\n",
							"#   container : nombre de contenedor en ADLS\r\n",
							"#   path : ruta donde se escribira el delta [\"abfss://contenedor@nombre_adls.dfs.core.windows.net/?/nombre_tabla\"]\r\n",
							"\r\n",
							"def write_delta(df, mode, container, path):\r\n",
							"\r\n",
							"    df.write.format(\"delta\") \\\r\n",
							"    .mode(mode) \\\r\n",
							"    .save(f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{path}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# ------------------------------------------------------------------------------------------\r\n",
							"# se lee delta\r\n",
							"# ------------------------------------------------------------------------------------------\r\n",
							"# parametros:\r\n",
							"#   container : nombre de contenedor en ADLS\r\n",
							"#   path : ruta donde se leera el delta [\"abfss://contenedor@nombre_adls.dfs.core.windows.net/?/nombre_tabla\"]\r\n",
							"\r\n",
							"def read_delta(container, path):\r\n",
							"    df = spark.read.format(\"delta\") \\\r\n",
							"    .load(f\"abfss://{container}@{storage_account}.dfs.core.windows.net/{path}\")\r\n",
							"    \r\n",
							"    return df"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/write_csv_source_to_landing')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "landing"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "kyndryl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "f517f8b2-75bc-4b47-b5cc-e3fe140ad597"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4516ad2-d1ef-463b-817c-b2a88c0b12bd/resourceGroups/rg-kyndryl/providers/Microsoft.Synapse/workspaces/synwkyndryl/bigDataPools/kyndryl",
						"name": "kyndryl",
						"type": "Spark",
						"endpoint": "https://synwkyndryl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/kyndryl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# se define parametros del notebook\r\n",
							"name_database = \"\"\r\n",
							"query = \"\"\r\n",
							"mode_csv = \"\"\r\n",
							"header_csv = \"\"\r\n",
							"delimiter_csv = \"\"\r\n",
							"container_csv = \"\"\r\n",
							"path_csv = \"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"%run Utils"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#leemos una query en azure sql\r\n",
							"df = getReadSQL(name_database, query)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#escribimos en formato csv a landing el dataframe que obtuvimos en read_sql()\r\n",
							"write_csv(df, mode_csv, header_csv, delimiter_csv,container_csv, path_csv)"
						],
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/write_delta_landing_to_staging')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "staging"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "kyndryl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e0e58102-7309-464e-aada-531b058862c0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4516ad2-d1ef-463b-817c-b2a88c0b12bd/resourceGroups/rg-kyndryl/providers/Microsoft.Synapse/workspaces/synwkyndryl/bigDataPools/kyndryl",
						"name": "kyndryl",
						"type": "Spark",
						"endpoint": "https://synwkyndryl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/kyndryl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run Utils"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#leemos un csv de landing\r\n",
							"df = getReadCSV(\"SalesOrderHeader\")"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# se añade marca de tiempo en que se inserto el dato\r\n",
							"df = df.withColumn(\"auditDate\", current_timestamp())"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#escribimos un dataframe en modo append a staging\r\n",
							"getWriteDelta(df, \"append\", \"staging\", \"SalesOrderHeader\")"
						],
						"outputs": [],
						"execution_count": 15
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/write_delta_staging_to_ods')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "ods"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "kyndryl",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "afe0aed2-3174-447a-aff1-c364bb5e6d07"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/e4516ad2-d1ef-463b-817c-b2a88c0b12bd/resourceGroups/rg-kyndryl/providers/Microsoft.Synapse/workspaces/synwkyndryl/bigDataPools/kyndryl",
						"name": "kyndryl",
						"type": "Spark",
						"endpoint": "https://synwkyndryl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/kyndryl",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"%run Utils"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#leemos delta de staging\r\n",
							"df = getReadDelta(\"staging\",\"SalesOrderHeader\")"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"dict_schema = { \r\n",
							"    \"SalesOrderID\": IntegerType(), \r\n",
							"    \"RevisionNumber\": ByteType(), \r\n",
							"    \"OrderDate\": TimestampType(), \r\n",
							"    \"DueDate\": TimestampType(),  \r\n",
							"    \"ShipDate\": TimestampType(), \r\n",
							"    \"Status\": ByteType(),\r\n",
							"    \"OnlineOrderFlag\": BooleanType(), \r\n",
							"    \"SalesOrderNumber\": StringType(), \r\n",
							"    \"PurchaseOrderNumber\": StringType(), \r\n",
							"    \"AccountNumber\": StringType(), \r\n",
							"    \"CustomerID\": IntegerType(), \r\n",
							"    \"ShipToAddressID\": IntegerType(), \r\n",
							"    \"BillToAddressID\": IntegerType(), \r\n",
							"    \"ShipMethod\": StringType(), \r\n",
							"    \"CreditCardApprovalCode\": StringType(), \r\n",
							"    \"SubTotal\": DecimalType(18,4), \r\n",
							"    \"TaxAmt\": DecimalType(18,4), \r\n",
							"    \"Freight\": DecimalType(18,4), \r\n",
							"    \"TotalDue\": DecimalType(18,4), \r\n",
							"    \"Comment\": StringType(), \r\n",
							"    \"rowguid\": StringType(), \r\n",
							"    \"ModifiedDate\": TimestampType(),\r\n",
							"    \"auditDate\": TimestampType()\r\n",
							"} "
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# se asigna el tipo de dato correcto al dataframe de staging\r\n",
							"for columna, tipo in dict_schema.items():\r\n",
							"    df = df.withColumn(columna, col(columna).cast(tipo))\r\n",
							"\r\n",
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"dfRefined = df \\\r\n",
							"            .withColumn(\"OnlineOrderFlag\", when(col(\"OnlineOrderFlag\") == \"false\", False) \\\r\n",
							"                                        .otherwise(True)) \\\r\n",
							"            .withColumn(\"Comment\", when(col(\"Comment\").isNull(), \"\") \\\r\n",
							"                                        .otherwise(col(\"Comment\")))\r\n",
							"#display(dfRefined)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# escribimos un dataframe en modo overwrite a ods\r\n",
							"getWriteDelta(dfRefined, \"overwrite\", \"ods\", \"SalesOrderHeader\")"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/kyndryl')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 5
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": false,
				"customLibraries": [
					{
						"name": "spark-mssql-connector_2.12-1.3.0-BETA.jar",
						"path": "synwkyndryl/libraries/spark-mssql-connector_2.12-1.3.0-BETA.jar",
						"containerName": "prep",
						"uploadedTimestamp": "0001-01-01T00:00:00+00:00",
						"type": "jar"
					}
				],
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}